# -*- coding: utf-8 -*-
"""Telco Customer Churn 022425.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wG3tnExBbyKBOfa--6tZeWscxgAbqUqZ
"""

pip install optuna

import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from imblearn.over_sampling import SMOTE

# Load the dataset
df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')

# Basic preprocessing
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(df['MonthlyCharges'], inplace=True)
df.drop('customerID', axis=1, inplace=True)

# Encode categorical variables
le = LabelEncoder()
for column in df.columns:
    if df[column].dtype == 'object':
        df[column] = le.fit_transform(df[column])

# Feature engineering
df['ChargePerMonthRatio'] = df['TotalCharges'] / (df['tenure'] + 1)  # Avoid division by zero
df['TenureContractInteraction'] = df['tenure'] * df['Contract']

# Define features (X) and target (y)
X = df.drop('Churn', axis=1)
y = df['Churn']

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

# Function to optimize LightGBM hyperparameters with Optuna
def objective(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',  # Optimize directly for AUC
        'boosting_type': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 50),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
        'max_depth': trial.suggest_int('max_depth', 5, 15),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 5.0),
        'n_estimators': trial.suggest_int('n_estimators', 50, 200)
    }

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    auc_scores = []

    for train_idx, val_idx in skf.split(X_balanced, y_balanced):
        X_train, X_val = X_balanced.iloc[train_idx], X_balanced.iloc[val_idx]
        y_train, y_val = y_balanced.iloc[train_idx], y_balanced.iloc[val_idx]

        train_data = lgb.Dataset(X_train, label=y_train)
        val_data = lgb.Dataset(X_val, label=y_val)

        model = lgb.train(params, train_data, num_boost_round=1000,
                         valid_sets=[val_data], callbacks=[lgb.early_stopping(stopping_rounds=10)])

        y_pred = model.predict(X_val)
        auc = roc_auc_score(y_val, y_pred)
        auc_scores.append(auc)

    return np.mean(auc_scores)

# Run Optuna optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# Get best parameters
best_params = study.best_params
print("Best parameters:", best_params)

# Train final model with best parameters
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced)

train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

final_model = lgb.train({**best_params, 'objective': 'binary', 'metric': 'auc'},
                        train_data, num_boost_round=1000, valid_sets=[test_data],
                        callbacks=[lgb.early_stopping(stopping_rounds=10)])

# Make predictions
y_pred = final_model.predict(X_test)
y_pred_binary = [1 if x > 0.5 else 0 for x in y_pred]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_binary)
auc_roc = roc_auc_score(y_test, y_pred)
print(f"Final Accuracy: {accuracy:.4f}")
print(f"Final AUC-ROC Score: {auc_roc:.4f}")

# Visualizations
plt.figure(figsize=(15, 10))

# 1. Correlation Matrix Heatmap
plt.subplot(2, 2, 1)
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, cmap='coolwarm', annot=False, center=0)
plt.title('Feature Correlations')

# 2. Churn Distribution Bar Plot (Original Dataset)
plt.subplot(2, 2, 2)
churn_counts = df['Churn'].value_counts()
sns.barplot(x=churn_counts.index, y=churn_counts.values, palette='Blues_d')
plt.title('Distribution of Churn (0 = No, 1 = Yes)')
plt.xlabel('Churn')
plt.ylabel('Count')

# 3. ROC Curve
plt.subplot(2, 2, 3)
fpr, tpr, _ = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")

# 4. Precision-Recall Curve
plt.subplot(2, 2, 4)
precision, recall, _ = precision_recall_curve(y_test, y_pred)
pr_auc = auc(recall, precision)
plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")

plt.tight_layout()
plt.show()

# Feature importance visualization
plt.figure(figsize=(10, 6))
lgb.plot_importance(final_model, max_num_features=10, importance_type='gain')
plt.title('Top 10 Features Driving Customer Churn')
plt.show()